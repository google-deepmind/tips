{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjlILc7jN0MO"
      },
      "source": [
        "Copyright 2025 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qs1CcsHIxtfx"
      },
      "outputs": [],
      "source": [
        "# @title TIPS Demo notebook\n",
        "\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "\n",
        "#  https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXG9RJf8U5Ti"
      },
      "source": [
        "### Imports and functions\n",
        "\n",
        "Please follow the installation guide before you run the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJgQoq1XgEP3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import os\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import Image\n",
        "import tensorflow_text\n",
        "from tips.pytorch import image_encoder\n",
        "from tips.pytorch import text_encoder\n",
        "from tips.scenic.utils import feature_viz\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "IMAGE_MEAN = (0, 0, 0)\n",
        "IMAGE_STD = (1.0, 1.0, 1.0)\n",
        "PATCH_SIZE = 14\n",
        "MAX_LEN = 64\n",
        "VOCAB_SIZE = 32000\n",
        "\n",
        "\n",
        "def load_image_bytes(file_name):\n",
        "  with open(file_name, 'rb') as fd:\n",
        "    image_bytes = io.BytesIO(fd.read())\n",
        "    pil_image = Image.open(image_bytes)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGE_MEAN, IMAGE_STD),\n",
        "    ])\n",
        "    input_tensor = transform(pil_image)\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "  return input_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LBtTQrRNfbi"
      },
      "source": [
        "### Configure the TIPS model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eMXX0d3Idr7"
      },
      "outputs": [],
      "source": [
        "# Set the input image shape and variant.\n",
        "image_size = 448  # @param {type: \"number\"}\n",
        "variant = 'S'  # @param ['S', 'B', 'L', 'So400m', 'g']\n",
        "\n",
        "# Add your images in this directory.\n",
        "image_dir = '../scenic/images/'  # @param {type: \"string\"}\n",
        "image_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "# The text inputs to be contrasted.\n",
        "text_inputs = [\n",
        "    'A ship',\n",
        "    'holidays',\n",
        "    'a toy dinosaur',\n",
        "    'Two astronauts',\n",
        "    'A streetview image of a fastfood restaurant',\n",
        "    'a cat',\n",
        "    'a dog',\n",
        "    'two cows',\n",
        "]\n",
        "\n",
        "# Add the checkpoints and tokenizer path.\n",
        "image_encoder_checkpoint = ''  # @param {type: \"string\"}\n",
        "text_encoder_checkpoint = ''  # @param {type: \"string\"}\n",
        "tokenizer_path = ''  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cuJRuZ2U8qZ"
      },
      "source": [
        "### Run Vision Encoder Inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK2JLKaR36jt"
      },
      "outputs": [],
      "source": [
        "# Load checkpoint.\n",
        "weights_image = dict(np.load(image_encoder_checkpoint, allow_pickle=False))\n",
        "for key in weights_image:\n",
        "  weights_image[key] = torch.tensor(weights_image[key])\n",
        "ffn_layer = 'swiglu' if variant == 'g' else 'mlp'\n",
        "\n",
        "embeddings_image, spatial_features = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Load the vision encoder.\n",
        "  model_image = image_encoder.vit_small(\n",
        "      img_size=image_size,\n",
        "      patch_size=PATCH_SIZE,\n",
        "      ffn_layer=ffn_layer,\n",
        "      block_chunks=0,\n",
        "      init_values=1.0,\n",
        "      interpolate_antialias=True,\n",
        "      interpolate_offset=0.0,\n",
        "  )\n",
        "  model_image.load_state_dict(weights_image)\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    input_batch = load_image_bytes(image_path)\n",
        "    output = model_image(input_batch)\n",
        "    cls_token = feature_viz.normalize(\n",
        "        output[0][0][0]\n",
        "    )  # Choose the first CLS token.\n",
        "    spatial_feature = torch.reshape(\n",
        "        output[2],\n",
        "        (1, int(image_size / PATCH_SIZE), int(image_size / PATCH_SIZE), -1),\n",
        "    )\n",
        "    spatial_features.append(spatial_feature)\n",
        "    embeddings_image.append(cls_token)"
      ]
    },
    {
      "metadata": {
        "id": "IFTiyxEKK5h1"
      },
      "cell_type": "markdown",
      "source": [
        "### Run Text Encoder Inference."
      ]
    },
    {
      "metadata": {
        "id": "P-YiddXKK5h1"
      },
      "cell_type": "code",
      "source": [
        "def get_text_config(v):\n",
        "  return {\n",
        "      'hidden_size': {'S': 384, 'B': 768, 'L': 1024, 'So400m': 1152, 'g': 1536}[\n",
        "          v\n",
        "      ],\n",
        "      'mlp_dim': {'S': 1536, 'B': 3072, 'L': 4096, 'So400m': 4304, 'g': 6144}[\n",
        "          v\n",
        "      ],\n",
        "      'num_heads': {'S': 6, 'B': 12, 'L': 16, 'So400m': 16, 'g': 24}[v],\n",
        "      'num_layers': {'S': 12, 'B': 12, 'L': 12, 'So400m': 27, 'g': 12}[v],\n",
        "  }\n",
        "\n",
        "\n",
        "with open(text_encoder_checkpoint, 'rb') as fin:\n",
        "  inbuffer = io.BytesIO(fin.read())\n",
        "np_weights_text = np.load(inbuffer, allow_pickle=False)\n",
        "\n",
        "weights_text = {}\n",
        "for key, value in np_weights_text.items():\n",
        "  weights_text[key] = torch.from_numpy(value)\n",
        "\n",
        "temperature = weights_text.pop('temperature')\n",
        "with torch.no_grad():\n",
        "  # Load the text encoder.\n",
        "  model_text = text_encoder.TextEncoder(\n",
        "      get_text_config(variant),\n",
        "      vocab_size=VOCAB_SIZE,\n",
        "  )\n",
        "  model_text.load_state_dict(weights_text)\n",
        "  tokenizer = text_encoder.Tokenizer(tokenizer_path)\n",
        "\n",
        "  text_ids, text_paddings = tokenizer.tokenize(text_inputs, max_len=MAX_LEN)\n",
        "  embeddings_text = model_text(\n",
        "      torch.from_numpy(text_ids), torch.from_numpy(text_paddings)\n",
        "  )\n",
        "  embeddings_text = feature_viz.normalize(embeddings_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mryeP_HySVZ"
      },
      "source": [
        "### Visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bapmyzSnBu8v"
      },
      "outputs": [],
      "source": [
        "for idx, embedding_image in enumerate(embeddings_image):\n",
        "  # Compute cosine similariy.\n",
        "  cos_sim = F.softmax(\n",
        "      ((embedding_image.unsqueeze(0) @ embeddings_text.T) / temperature), dim=-1\n",
        "  )\n",
        "  label_idxs = torch.argmax(cos_sim, axis=-1)\n",
        "  cos_sim_max = torch.max(cos_sim, axis=-1)\n",
        "  label_predicted = text_inputs[label_idxs[0]]\n",
        "  similarity = cos_sim_max.values[0]\n",
        "\n",
        "  # Visualize the results.\n",
        "  pca_obj = feature_viz.PCAVisualizer(spatial_features[idx])\n",
        "  image_pca = pca_obj(spatial_features[idx])[0]\n",
        "\n",
        "  with open(image_paths[idx], 'rb') as f:\n",
        "    image = Image.open(f)\n",
        "    image = image.resize((image_size, image_size), Image.Resampling.BILINEAR)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "  media.show_images(\n",
        "      [image, image_pca],\n",
        "      width=image_size,\n",
        "      titles=[\n",
        "          'Input image',\n",
        "          f'{label_predicted},  prob: {similarity*100:.1f}%',\n",
        "      ],\n",
        "  )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XXG9RJf8U5Ti",
        "5cuJRuZ2U8qZ"
      ],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "TIPS Demo",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "18Aq6CW6qPm4daIQO37NHN46EYk6cx1j2",
          "timestamp": 1676239803755
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
